Общие замечания по утилитам.
sort можно тюнить при помощи -S <memsize> и -T <temp folder>. Второе особенно полезно, когда раздел /tmp маленький - тогда надо заводить папку в крупной партиции.
Удобно следить за работой, встраивая в конвейер монитор числа обработанных строк `pv -l`. Ну и time, конечно.
Олсо, я использую sponge (из moreutils), чтобы не плодить временные файлы.

Приведём временные зоны
	Сервер sci-hub живёт в часовом поясе Москвы. В частности, 9 сентября 2017, когда sci-hub был разбанен, первые запросы из Москвы пошли в 20:23. Пост о разблокировке был <a href="https://vk.com/wall-36928352_16029">опубликован</a> в vk в 20:24. Происходил ли на сервере переход на летнее/зимнее время, мне обнаружить не удалось, но Александра Элбакян в комментариях упоминала, что не должен был.

	Операция вычисления временной зоны долгая (и в первой версии включала ненужный этап геокодирования внешними серисами - яндексом т.е.). Однако места и координаты часто повторяются - так что можно посчитать timezone для сравнительно небольшого числа уникальных мест, а затем просто подставлять полученную временную зону

	По координатам легко получить timezone (питоновская библиотека timezonefinder). По местам сложнее, может потребоваться геокодирование
	zcat 2017.statistics.1016.j.tab.gz scihub_stats_2017.tsv.gz | cut -d $'\t' -f5,6,7,8 | sort -t $'\t' -u > results_secondary/all_places_and_coords.tsv

	Чтобы упростить задачу, отберем по странам те, где только одна временная зона. Это можно сделать переведя название страны в код, а по коду получив все временные зоны при помощи pytz. country_codes.tsv скопирован с https://www.geoips.com/en/resources/countries/country-profile/country/bo и поправлен руками. Временные зоны некоторых стран (Косово, например) вбиты руками. Ну и алиасы названий стран, конечно
		import pytz
		with open('results_secondary/country_timezones.tsv', 'w') as fw:
		  with open('supplementary_data/country_codes.tsv') as f:
		    for l in f:
		      country, code, *rest = l.strip().split("\t")
		      zones = pytz.country_timezones(code)
		      if len(zones) == 1:
		        print(country, zones[0], sep="\t", file=fw)

	Кое-где, хотя зон несколько, весь 2017 они совпадают (Europe/Berlin и Europe/Busingen - как пример; так что всем немецким городам можно поставить эту зону - прямо в country_timezones можем упомянуть - заодно сгруппируем временные зоны, где можно)
	Например, проверим:

	require 'tzinfo'
	zones = ['Europe/Berlin', 'Europe/Busingen']
	zones.map!{|zone| TZInfo::Timezone.get(zone) }
	365.times.all?{|yday|
	  utc = Time.new(2017,1,1) + yday * 3600*24
	  zones.map{|zone| zone.period_for_utc(utc) }.uniq.size == 1
	}
	# => true - т.е. на протяжение всего года интервал временной зоны одинаков для обоих зон Германии
	Аналогичная история - в Палестине (Asia/Hebron и Asia/Gaza совпадают), в Украине (Europe/Kiev, Europe/Uzhgorod, Europe/Zaporozhye), в Узбекстане (Asia/Samarkand, Asia/Tashkent)

	Остается проверить товарищей, у которых нет координаты, и у которых по стране зону не определить. К счастью, таких "товарищей" немного, всего штук 50. Из них многие неразрешимы. Например, для неопределенного города Канады без координат, ничего нельзя поделать: там слишком большой разброс временных зон.
		places = File.readlines('results_secondary/all_places_and_coords.tsv').map{|l| l.chomp.split("\t") }
		country_timezones = File.readlines('results_secondary/country_timezones.tsv').map{|l| l.chomp.split("\t") }.to_h
		places.reject{|row|
		  country,city,lat,lng = *row
		  has_coords = (lat != 'N/A') && (lng != 'N/A')
		  has_coords || country_timezones.has_key?(country)
		}.each{|row|
		  puts row.join("\t")
		}

	Остаётся некоторое количество отдельных городов, для которых мы через геокодер или руками вбиваем координаты. Временную зону по координатам можно узнать, например, таким образом:
		from timezonefinder import TimezoneFinder
		tf = TimezoneFinder()
		timezone=tf.certain_timezone_at(lat=lat, lng=lng)


	Дальше кодируем по принципу: проверяем зону по стране; затем по городу (вбитых вручную зон городов - считанные штуки); затем по координате. Координаты, которые требуется посчитать - выпишем координаты в файл coords_unknown_timezone.tsv и предподсчитаем зоны 
		File.open('coords_unknown_timezone.tsv', 'w'){|fw|
		  places = File.readlines('results_secondary/all_places_and_coords.tsv').map{|l| l.chomp.split("\t") }
		  country_timezones = File.readlines('results_secondary/country_timezones.tsv').map{|l| l.chomp.split("\t") }.to_h
		  city_timezones = File.readlines('supplementary_data/city_timezones.tsv').map{|l| country, city, tz = l.chomp.split("\t"); [[country,city], tz] }.to_h
		  places.select{|row|
		    country,city,lat,lng = *row
		    has_coords = (lat != 'N/A') && (lng != 'N/A')
		    !country_timezones.has_key?(country) && !city_timezones.has_key?([country, city]) && has_coords  
		  }.each{|row|
		    fw.puts row.join("\t")
		  }
		}

		cat coords_unknown_timezone.tsv | python3 timezone_by_coords.py > results_secondary/timezone_by_coords.tsv

	Теперь, наконец, мы можем привести логи к локальному времени.

	zcat 2017.statistics.1016.j.tab.gz scihub_stats_2017.tsv.gz | ruby initial_prepare.rb >refined/in_local_time.tsv 2>refined/unmappable.tsv
	sort -t $'\t' -u -k1,1 -k2,2 -k15,15 -k16,16 refined/in_local_time.tsv | sponge refined/in_local_time.tsv
	sort -t $'\t' -u -k1,1 -k2,2 -k3  refined/unmappable.tsv | sponge refined/unmappable.tsv

	Немного статистики:
		wc -l refined/*
		  189972487 refined/in_local_time.tsv
		    4370487 refined/unmappable.tsv

	Не приводится к локальному времени чуть больше 2% записей, не запредельно много. В основном они нам неинтересны, но их можно использовать как некоторый контроль того, что геокодер, которым пользуется sci-hub работает одинаково хорошо.

Разложим данные по странам-городам. На предыдущем шаге мы отсортировали их по географии и по времени, так что теперь мы можем разбить на отдельные города в один проход по файлу, держа за раз один файловый дескриптор
	cat refined/in_local_time.tsv | ruby chunk_cities.rb

По каждому городу подсчитаем статистики вида "сколько закачек пришлось на интервал времени". 
Обозначения:
yday - день года; wday - день недели; hour - очевидно, daytime_10 - время дня, измеренное в десятиминутных интервалах; weekhour (или weektime_60) и weekhour_10 (или weektime_10) - это время с начала недели, измеренное в часах или в десятиминутках.
mean_daytime - центр масс дня
Заодно посчитаем сколько скачиваний из страны имело тот или иной doi-префикс
В результате получим папки ./counts/cities/ и ./rates/cities/

	ruby extract_city_counts.rb

Просуммируем значения по странам, доли тоже пересчитаем - см ./counts/countries/ и ./rates/countries/
	ruby aggregate_countries.rb

Сгенерим тепловые карты
Для всех стран:
	ruby extract_heatmaps.rb
Для конкретного города:
	mkdir -p heatmaps/custom
	cat by_city/Russia/Moskva.tsv | ruby extract_custom_heatmap.rb all heatmaps/custom/Moscow


Данные для хороплетов:
	( echo $'country\tcount'; zcat 2017.statistics.1016.j.tab.gz scihub_stats_2017.tsv.gz | cut -f5 -d $'\t' | uniq -c | sed -re 's/^\s*([0-9]+)\s+(.+)/\2\t\1/' | sort -t $'\t' -k1,1 ) > country_level_counts/country_counts.tsv
	( echo $'country\tcount'; zcat scihub_stats_2017.tsv.gz 2017.statistics.1016.j.tab.gz | awk --field-separator=$'\t' -e '($2 ~ /^10\.1109\//){print $0}' | cut -d $'\t' -f5 | sort | uniq -c | sed -re 's/^\s*([0-9]+)\s+(.+)/\2\t\1/' | sort -t $'\t' -k1,1 ) > country_level_counts/country_counts_ieee.tsv
	( echo $'country\tcount'; zcat scihub_stats_2017.tsv.gz 2017.statistics.1016.j.tab.gz | awk --field-separator=$'\t' -e '($2 ~ /^(10\.1021|10\.1039)\//){print $0}' | cut -d $'\t' -f5 | sort | uniq -c | sed -re 's/^\s*([0-9]+)\s+(.+)/\2\t\1/' | sort -t $'\t' -k1,1 ) > country_level_counts/country_counts_acs_rsc.tsv

	# Эльзивер имеет два doi-префикса:
	( echo $'country\tcount'; zcat scihub_stats_2017.tsv.gz 2017.statistics.1016.j.tab.gz | awk --field-separator=$'\t' -e '($2 ~ /^(10\.1016|10\.1006)\//){print $0}' | cut -d $'\t' -f5 | sort | uniq -c | sed -re 's/^\s*([0-9]+)\s+(.+)/\2\t\1/' | sort -t $'\t' -k1,1 ) > country_level_counts/country_counts_elsevier.tsv

	# Что происходило в январе:
	( echo $'country\tcount'; zcat 2017.statistics.1016.j.tab.gz scihub_stats_2017.tsv.gz | awk --field-separator=$'\t' -e '(($2 ~ /^(10\.1016|10\.1006)\//) && ($1 ~ /^2017-(01|02-0)/)){print $0}' | cut -d $'\t' -f5 | sort | uniq -c | sed -re 's/^\s*([0-9]+)\s+(.+)/\2\t\1/' | sort -t $'\t' -k1,1 ) > country_level_counts/country_counts_elsevier_january_upto_09feb.tsv

	( echo $'country\tcount'; zcat 2017.statistics.1016.j.tab.gz scihub_stats_2017.tsv.gz | awk --field-separator=$'\t' -e '($1 ~ /^2017-(01|02-0)/){print $0}' | cut -d $'\t' -f5 | sort | uniq -c | sed -re 's/^\s*([0-9]+)\s+(.+)/\2\t\1/' | sort -t $'\t' -k1,1 ) > country_level_counts/country_counts_january_upto_09feb.tsv

Если мы строим хороплеты в R (на самом деле, не строим, потому что R - отстой), то нам придётся преобразовать названия стран в ISO-коды
	# join --header -t $'\t' -1 1 -2 2 country_level_counts/country_counts.tsv supplementary_data/iso3_codes.tsv > country_level_counts/country_counts_coded.tsv 
	# join --header -t $'\t' -1 1 -2 2 country_level_counts/country_counts_ieee.tsv supplementary_data/iso3_codes.tsv > country_level_counts/country_counts_coded_ieee.tsv

	# join --header -t $'\t' <( head -1 country_level_counts/country_counts.tsv; tail -n+2 country_level_counts/country_counts.tsv | sort -t $'\t' -k1,1  ) <( head -1 population.tsv; tail -n+2 population.tsv | sort -t $'\t' -k1,1 ) > country_level_counts/country_counts_w_population.tsv
	# join --header -t $'\t' <( head -1 country_level_counts/country_counts_ieee.tsv; tail -n+2 country_level_counts/country_counts.tsv | sort -t $'\t' -k1,1  ) <( head -1 population.tsv; tail -n+2 population.tsv | sort -t $'\t' -k1,1 ) > country_level_counts/country_counts_ieee_w_population.tsv

	cat country_level_counts/country_counts_coded.tsv | ./draw_choropleth.r choropleth.png
	cat country_level_counts/country_counts_coded_ieee.tsv | ./draw_choropleth.r choropleth_ieee.png

Посчитаем число скачиваний с шагом в полградуса по широте и долготе (не используем)
	zcat 2017.statistics.1016.j.tab.gz scihub_stats_2017.tsv.gz | cut -d $'\t' -f7,8 | fgrep -v 'N/A' | ruby hotspots.rb 0.5 > results_secondary/hotspots/hotspots_0.5.tsv
	zcat 2017.statistics.1016.j.tab.gz scihub_stats_2017.tsv.gz | cut -d $'\t' -f2,7,8 | grep -Pe '^10\.1109/' | cut -d $'\t' -f2,3 | fgrep -v 'N/A' | ruby hotspots.rb 0.5 > results_secondary/hotspots/hotspots_0.5_ieee.tsv
	zcat 2017.statistics.1016.j.tab.gz scihub_stats_2017.tsv.gz | cut -d $'\t' -f2,7,8 | grep -Pe '(^10\.1016/)|(^10\.1006/)' | cut -d $'\t' -f2,3 | fgrep -v 'N/A' | ruby hotspots.rb 0.5 > results_secondary/hotspots/hotspots_0.5_elsevier.tsv
	zcat 2017.statistics.1016.j.tab.gz scihub_stats_2017.tsv.gz | cut -d $'\t' -f2,7,8 | grep -Pe '^10\.1021/' | cut -d $'\t' -f2,3 | fgrep -v 'N/A' | ruby hotspots.rb 0.5 > results_secondary/hotspots/hotspots_0.5_acs.tsv
	zcat 2017.statistics.1016.j.tab.gz scihub_stats_2017.tsv.gz | cut -d $'\t' -f2,7,8 | grep -Pe '^10\.1111/' | cut -d $'\t' -f2,3 | fgrep -v 'N/A' | ruby hotspots.rb 0.5 > results_secondary/hotspots/hotspots_0.5_wiley.tsv

	# рисуем эти карты (эти картинки не используем)
	cat results_secondary/hotspots/hotspots_0.5.tsv | ./draw_dowloads_map.r all_downloads_0.5.png
	cat results_secondary/hotspots/hotspots_0.5_ieee.tsv | ./draw_dowloads_map.r all_downloads_ieee.png


Оценим рост числа уникальных пользователей встреченных в конкретный день и к текущему моменту (здесь мы должны подавать на вход отсортированный по дате поток, так что нельзя просто cat-ом смешать города)
	cat by_city/Russia/Moskva.tsv | ruby audience_growth.rb
	ruby audience_growth_runner.rb | bash

(не сработало)
Вычислим центры масс и координаты в каждом городе-стране ради дальнейшего анализа корреляции с широтой
	ruby country_central_coords.rb
	Агрегируем mean_daytime по месяцам как квартили mean daytime
	ruby aggregate_latlng_w_meandaytimes.rb > results_secondary/latlng_meantime.tsv


Посмотрим на доступность сервиса в разное время недели (в UTC), чтобы понять, нужен ли корректирующий коэффициент для разного времени, оценить масштаб ошибки.
	zcat 2017.statistics.1016.j.tab.gz scihub_stats_2017.tsv.gz | cut -f1 | ruby correction_on_availability.rb > results_secondary/availability.tsv
	time zcat 2017.statistics.1016.j.tab.gz scihub_stats_2017.tsv.gz | cut -d $'\t' -f1,5 | fgrep -w 'Russia' | cut -f1 | ruby correction_on_availability.rb > results_secondary/availability_Russia.tsv 
	cut -f4 results_secondary/availability.tsv | ruby -e 'min,max=readlines.map(&:to_f).minmax; puts [min,max, max/min].join("\t")'
	cut -f4 results_secondary/availability_Russia.tsv | ruby -e 'min,max=readlines.map(&:to_f).minmax; puts [min,max, max/min].join("\t")'

Вытянем названия статей по doi из OpenCitations (не сработало: opencitations оказался недостаточно большим)
	Вытащим doi-префиксы издателей. Внимательно! некоторые doi-префиксы относятся сразу к нескольким издателям. Так я путал 10.1111 у Wiley Online и какого-то маленького корейского общества ботаников - и удивлялся, откуда такой интерес к ботанике.
	Попытка номер раз - https://figshare.com/articles/title/4172382
	Попытка номер два - query.wikidata.org:
	SELECT ?publisher ?publisherLabel ?doi WHERE {
	  SERVICE wikibase:label { bd:serviceParam wikibase:language "[AUTO_LANGUAGE],en". }
	  ?publisher wdt:P1662 ?doi.
	}

	Вычленим по doi названия статей и журналов:
	cat opencitations/corpus_id/*/*.json | jq -c '."@graph"[] | select(.type=="doi") | {"gid": .iri,"doi": .id}' | ruby -rjson -e '$stdin.each_line{|l| obj = JSON.parse(l); doi = obj["doi"].gsub(/^https:\/\/doi\.org\//i,"").gsub(/^\s*doi(:\s*|\s+|\s*:\s*)/i, ""); puts [obj["gid"], doi].join("\t") }' | sort -t $'\t' -k1,1 > opencitations/gid_doi.tsv

	Выделим иерархию статья-выпуск-журнал итп, чтобы потом каждый объект поднять до верхнего уровня

	cat opencitations/corpus_br/*/*.json | jq -c '."@graph"[] | {iri, part_of}' | ruby -rjson -e '$stdin.each_line{|l| obj=JSON.parse(l); puts obj.values_at("iri", "part_of").join("\t") }' | sort -t $'\t' -k1,1 > opencitations/resource_hierarchy.tsv

	cat opencitations/resource_hierarchy.tsv | ruby -e 'def get_root(node) $parent[node] ? get_root($parent[node]) : node; end; $parent = $stdin.readlines.map{|l| k,v=l.chomp.split("\t",2); [k,v.empty? ? nil : v] }.to_h; $parent.each_key{|node| puts [node,get_root(node)].join("\t") }' | sort -t $'\t' -k1,1 > opencitations/resource_hierarchy_root.tsv

	cat opencitations/corpus_br/*/*.json | jq -c '."@graph"[] | {iri, title, year}' | ruby -rjson -e '$stdin.each_line{|l| obj=JSON.parse(l); puts [obj["iri"], obj["title"]&.gsub(/\s+/, " "), obj["year"]&.gsub(/\s+/, " ")].join("\t") }' | sort -t $'\t' -k1,1 > opencitations/title_year.tsv

	cat opencitations/corpus_br/*/*.json | jq -c '."@graph"[] | {iri, identifier}' | ruby -rjson -e '$stdin.each_line{|l| obj=JSON.parse(l); [obj["identifier"]].flatten.compact.each{|id| puts [id, obj["iri"]].join("\t") } }' | sort -t $'\t' -k1,1 > opencitations/resource_for_gid.tsv

	join -t $'\t' opencitations/gid_doi.tsv opencitations/resource_for_gid.tsv | sort -t $'\t' -k3,3 | join -t $'\t' -1 3 - opencitations/title_year.tsv | join -t $'\t' - opencitations/resource_hierarchy_root.tsv | sort -t $'\t' -k6,6 | join -t $'\t' -1 6 - opencitations/title_year.tsv | cut -d $'\t' -f4- | sort -t $'\t' -k1,1 > opencitations/doi_title_year_journal_year.tsv

Вспомогательный скрипт:
Узнаем, когда переводы часов в Марокко и Иране:
	ruby time_offset_periods.rb Asia/Tehran
	ruby time_offset_periods.rb Africa/Casablanca
